---
layout:     post
title:      "潜在语义分析"
subtitle:   "Latent semantic analysis"
date:       2018-01-20 12:00:00
author:     "YangLong"
header-img: "img/spiderman/post08.jpg"
header-mask: 0.3
catalog:    true 
multilingual: false  
tags:
    - 自然语言处理 
---
## LSA *Latent semantic analysis*  *潜在语义分析*    
 其核心思想是把我们所拥有的文档-术语矩阵分解成相互独立的文档-主题矩阵和主题-术语矩阵,其主要方法是使用SVD奇异值分解来进行
 
 #### Step.1 生成文档-术语模型 
 如果在词汇表中给出 m 个文档和 n 个单词，我们可以构造一个 m×n 的矩阵 A，其中每行代表一个文档，每列代表一个单词。在 LSA 的最简单版本中，每一个条目可以简单地是第 j 个单词在第 i 个文档中出现次数的原始计数。然而，在实际操作中，原始计数的效果不是很好，因为它们无法考虑文档中每个词的权重。因此，LSA 模型通常用 tf-idf 得分代替文档-术语矩阵中的原始计数。tf-idf，即词频-逆文本频率指数，为文档 i 中的术语 j 分配了相应的权重
  <img width='400px' src='https://raw.githubusercontent.com/ylhelloworld/resource/master/Image/20190415_lsa_001.jpeg'/> 
- 术语出现在文档中的频率越高，则其权重越大；  
- 同时，术语在语料库中出现的频率越低，其权重越大。

#### Step.2 SVD降维  
维可以使用截断 SVD 来执行。SVD，即奇异值分解，是线性代数中的一种技术。该技术将任意矩阵 M 分解为三个独立矩阵的乘积：M=U*S*V，其中 S 是矩阵 M 奇异值的对角矩阵。很大程度上，截断 SVD 的降维方式是：选择奇异值中最大的 t 个数，且只保留矩阵 U 和 V 的前 t 列。在这种情况下，t 是一个超参数，我们可以根据想要查找的主题数量进行选择和调整。  
<img width='400px' src='https://raw.githubusercontent.com/ylhelloworld/resource/master/Image/20190415_lsa_002.png'/>

-  U∈ℝ^（m⨉t）是我们的文档-主题矩阵
-  V∈ℝ^（n⨉t）则成为我们的术语-主题矩阵
。在矩阵 U 和 V 中，每一列对应于我们 t 个主题当中的一个。在 U 中，行表示按主题表达的文档向量；在 V 中，行代表按主题表达的术语向量。

#### Step.3 评估 
<img width='400px' src='https://raw.githubusercontent.com/ylhelloworld/resource/master/Image/20180415_lsa_03.png'/>

用余弦相似度等度量来评估以下指标：
    不同文档的相似度
    不同单词的相似度
    术语（或「queries」）与文档的相似度（当我们想要检索与查询最相关的段落，即进行信息检索时，这一点将非常有用）
>  *余弦相似度*  将向量根据坐标值，绘制到向量空间中。如最常见的二维空间。求得他们的夹角，并得出夹角对应的余弦值，此余弦值就可以用来表征，这两个向量的相似性。夹角越小，余弦值越接近于1，它们的方向更加吻合，则越相似。

每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说 stock 和 market 可以放在一类，因为他们老是出现在一起，real 和 estate 可以放在一类，dads，guide 这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词

### 总结  
  - LSA 方法快速且高效 
  - 缺乏可解释的嵌入（我们并不知道主题是什么，其成分可能积极或消极，这一点是随机的）
  - 需要大量的文件和词汇来获得准确的结果
  - 表征效率低

#### LSI(Latent Semantic Indexing)
